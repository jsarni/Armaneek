{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoriel Source : https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, paths):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "        self.tokenizer.train(trainer, paths)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(\"../dataset/transformers_train/cult/\").glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPE_token()\n",
    "\n",
    "tokenizer.bpe_train(paths)\n",
    "\n",
    "save_path = \"../dataset/transformers_train/tokenized_data/cult/\"\n",
    "tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "for filename in paths:\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        x = f.read()\n",
    "    single_string += x + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 50\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "    examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "\n",
    "for ex in examples:\n",
    "    inputs.append(ex[:-1])\n",
    "    labels.append(ex[1:])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juba/anaconda3/envs/nlp/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5773/5773 [==============================] - 2565s 444ms/step - loss: 6.2072 - output_1_loss: 6.2072 - output_1_accuracy: 0.1448 - output_2_1_accuracy: 0.0017 - output_2_2_accuracy: 0.0014 - output_2_3_accuracy: 0.0017 - output_2_4_accuracy: 0.0016 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0021 - output_2_8_accuracy: 0.0018 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0018 - output_2_12_accuracy: 0.0018\n",
      "Epoch 2/10\n",
      "5773/5773 [==============================] - 2576s 446ms/step - loss: 5.4637 - output_1_loss: 5.4637 - output_1_accuracy: 0.1865 - output_2_1_accuracy: 0.0016 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0014 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0013 - output_2_6_accuracy: 0.0017 - output_2_7_accuracy: 0.0016 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0017 - output_2_11_accuracy: 0.0014 - output_2_12_accuracy: 0.0018\n",
      "Epoch 3/10\n",
      "5773/5773 [==============================] - 2576s 446ms/step - loss: 5.1508 - output_1_loss: 5.1508 - output_1_accuracy: 0.2048 - output_2_1_accuracy: 0.0015 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0016 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0015 - output_2_10_accuracy: 0.0016 - output_2_11_accuracy: 0.0014 - output_2_12_accuracy: 0.0017\n",
      "Epoch 4/10\n",
      "5773/5773 [==============================] - 2577s 446ms/step - loss: 4.9244 - output_1_loss: 4.9244 - output_1_accuracy: 0.2186 - output_2_1_accuracy: 0.0015 - output_2_2_accuracy: 0.0016 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0016 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0015 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0014 - output_2_12_accuracy: 0.0015\n",
      "Epoch 5/10\n",
      "5773/5773 [==============================] - 2581s 447ms/step - loss: 4.7395 - output_1_loss: 4.7395 - output_1_accuracy: 0.2297 - output_2_1_accuracy: 0.0015 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0016 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0019 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0015 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n",
      "Epoch 6/10\n",
      "5773/5773 [==============================] - 2582s 447ms/step - loss: 4.5768 - output_1_loss: 4.5768 - output_1_accuracy: 0.2401 - output_2_1_accuracy: 0.0015 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0016 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n",
      "Epoch 7/10\n",
      "5773/5773 [==============================] - 2582s 447ms/step - loss: 4.4242 - output_1_loss: 4.4242 - output_1_accuracy: 0.2508 - output_2_1_accuracy: 0.0015 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0015 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n",
      "Epoch 8/10\n",
      "5773/5773 [==============================] - 2573s 446ms/step - loss: 4.2771 - output_1_loss: 4.2771 - output_1_accuracy: 0.2612 - output_2_1_accuracy: 0.0016 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0015 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n",
      "Epoch 9/10\n",
      "5773/5773 [==============================] - 2562s 444ms/step - loss: 4.1320 - output_1_loss: 4.1320 - output_1_accuracy: 0.2724 - output_2_1_accuracy: 0.0016 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0013 - output_2_4_accuracy: 0.0015 - output_2_5_accuracy: 0.0015 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0016 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n",
      "Epoch 10/10\n",
      "5773/5773 [==============================] - 2565s 444ms/step - loss: 3.9879 - output_1_loss: 3.9879 - output_1_accuracy: 0.2845 - output_2_1_accuracy: 0.0016 - output_2_2_accuracy: 0.0015 - output_2_3_accuracy: 0.0014 - output_2_4_accuracy: 0.0015 - output_2_5_accuracy: 0.0016 - output_2_6_accuracy: 0.0018 - output_2_7_accuracy: 0.0015 - output_2_8_accuracy: 0.0016 - output_2_9_accuracy: 0.0016 - output_2_10_accuracy: 0.0015 - output_2_11_accuracy: 0.0015 - output_2_12_accuracy: 0.0014\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "history = model.fit(dataset, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_gpt2_cult/tokenizer_config.json',\n",
       " './model_gpt2_cult/special_tokens_map.json',\n",
       " './model_gpt2_cult/vocab.json',\n",
       " './model_gpt2_cult/merges.txt',\n",
       " './model_gpt2_cult/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "import os\n",
    "output_dir = '../model_gpt2_cult/'\n",
    "# creating directory if it is not present\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "# save model and model configs\n",
    "model.save_pretrained(output_dir)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
